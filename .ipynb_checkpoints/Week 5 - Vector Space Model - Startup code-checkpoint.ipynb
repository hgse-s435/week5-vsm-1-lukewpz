{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import os\n",
    "import glob\n",
    "os.chdir('/Users/peizhiwen/Documents/GitHub/week5-vsm-1-lukewpz/')\n",
    "text_file = glob.glob('./Papers/paper*.txt')\n",
    "print(len(text_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "for text in text_file:\n",
    "    file= open(text,'r',encoding='utf-8')\n",
    "    documents.append(file.read())\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50043 39099\n",
      "41110 35334\n",
      "49177 42214\n",
      "32277 27511\n",
      "40387 33988\n",
      "45258 41978\n",
      "40655 32070\n",
      "31574 27787\n",
      "42046 37371\n",
      "46761 42033\n",
      "47377 42660\n",
      "44037 39651\n",
      "37214 32315\n",
      "47851 40777\n",
      "42617 34774\n",
      "45724 39581\n",
      "47845 43713\n"
     ]
    }
   ],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "docs = []\n",
    "for document in documents:\n",
    "    print(len(document), end=' ')\n",
    "    abstract = document.index('abstract', 0, 8000)\n",
    "    reference = document.rfind('reference')\n",
    "    newtext = document[abstract:reference]\n",
    "    docs.append(newtext)\n",
    "    print(len(newtext[abstract:reference]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a regex:\n",
    "#import re\n",
    "#d1 = document[0]\n",
    "#body = re.findall(r'abstract(.*?)reference', d1, re.DOTALL)\n",
    "#print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "\n",
    "#option 1:\n",
    "#nocarriage = []\n",
    "#for selecttext in selecttexts:\n",
    "#    newselect = selecttext.replace('\\n', ' ')\n",
    "#    nocarriage.append(newselect)\n",
    "#print(nocarriage[0][:1000])\n",
    "#for i in nocarriage:\n",
    "#    print(len(i))\n",
    "\n",
    "#option 2:\n",
    "#for i in range(len(documents)):\n",
    "#    documents[i] = documents[i].replace('\\n',' ')\n",
    "\n",
    "#option 3:\n",
    "for i, doc in enumerate(docs):\n",
    "    docs[i] = doc.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39318 39318\n",
      "35514 35514\n",
      "42621 42621\n",
      "28206 28206\n",
      "34778 34778\n",
      "42251 42251\n",
      "32734 32734\n",
      "28134 28134\n",
      "37649 37649\n",
      "42253 42253\n",
      "42978 42978\n",
      "40032 40032\n",
      "32762 32762\n",
      "41302 41302\n",
      "35102 35102\n",
      "39947 39947\n",
      "44059 44059\n",
      "abstract mind wandering  defined as shifts in attention from task related processing to task unrelated thoughts  is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts  including learning  we propose that next generation learning technologies should have some mechanism to detect and respond to mind wandering in real time  towards this end  we developed a technology that automatically detects mind wandering from eye gaze during learning from instructional texts  when mind wandering is detected  the technology intervenes by posing just in time questions and encouraging re reading as needed  after multiple rounds of iterative refinement  we summatively compared the technology to a yoked control in an experiment with 104 participants  the key dependent variable was performance on a post reading comprehension assessment  our results suggest that the technology was successful in correcting comprehension deficits attributed to mind wandering \n"
     ]
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(len(doc), end=' ')\n",
    "    for punc in punctuation:\n",
    "        docs[i]=doc.replace(punc, ' ')\n",
    "        doc = docs[i]\n",
    "    print(len(docs[i]))\n",
    "print(docs[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract mind wandering  defined as shifts in attention from task related processing to task unrelated thoughts  is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts  including learning  we propose that next generation learning technologies should have some mechanism to detect and respond to mind wandering in real time  towards this end  we developed a technology that automatically detects mind wandering from eye gaze during learning from instructional texts  when mind wandering is detected  the technology intervenes by posing just in time questions and encouraging re reading as needed  after multiple rounds of iterative refinement  we summatively compared the technology to a yoked control in an experiment with     participants  the key dependent variable was performance on a post reading comprehension assessment  our results suggest that the technology was successful in correcting comprehension deficits attributed to mind wandering \n"
     ]
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "numbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "for i, doc in enumerate(docs):\n",
    "    for num in numbers:\n",
    "        docs[i]=doc.replace(num, ' ')\n",
    "        doc = docs[i]\n",
    "print(docs[0][:1000])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5759 3428\n",
      "5427 3348\n",
      "6339 3913\n",
      "4056 2503\n",
      "5246 3230\n",
      "6688 3991\n",
      "5052 3018\n",
      "4222 2656\n",
      "5489 3443\n",
      "6394 3931\n",
      "6569 3972\n",
      "6080 3848\n",
      "4881 3228\n",
      "6305 3842\n",
      "5516 3327\n",
      "6006 3967\n",
      "6442 4182\n",
      "17\n",
      "['abstract', 'mind', 'wandering', 'defined', 'shifts', 'attention', 'task', 'related', 'processing', 'task', 'unrelated', 'thoughts', 'ubiquitous', 'phenomenon', 'negative', 'influence', 'performance', 'productivity', 'many', 'contexts', 'including', 'learning', 'propose', 'next', 'generation', 'learning', 'technologies', 'mechanism', 'detect', 'respond', 'mind', 'wandering', 'real', 'time', 'towards', 'end', 'developed', 'technology', 'automatically', 'detects', 'mind', 'wandering', 'eye', 'gaze', 'learning', 'instructional', 'texts', 'mind', 'wandering', 'detected', 'technology', 'intervenes', 'posing', 'time', 'questions', 'encouraging', 're', 'reading', 'needed', 'multiple', 'rounds', 'iterative', 'refinement', 'summatively', 'compared', 'technology', 'yoked', 'control', 'experiment', 'participants', 'key', 'dependent', 'variable', 'performance', 'post', 'reading', 'comprehension', 'assessment', 'results', 'suggest', 'technology', 'successful', 'correcting', 'comprehension', 'deficits', 'attributed', 'mind', 'wandering', 'd', 'sigma', 'specific', 'conditions', 'thereby', 'highlighting', 'potential', 'improve', 'learning', 'attending', 'attention', 'keywords', 'mind', 'wandering', 'gaze', 'tracking', 'student', 'modeling', 'attentionaware', 'introduction', 'despite', 'best', 'efforts', 'write', 'clear', 'engaging', 'paper', 'chances', 'high', 'within', 'next', 'pages', 'might', 'fall', 'prey', 'referred', 'zoning', 'daydreaming', 'mind', 'wandering', 'despite', 'best', 'intention', 'concentrate', 'paper', 'point', 'attention', 'might', 'drift', 'away', 'unrelated', 'thoughts', 'lunch', 'childcare', 'upcoming', 'trip', 'prediction', 'based', 'negative', 'cynical', 'opinion', 'reader', 'reviewer', 'read', 'review', 'papers', 'known', 'attentional', 'control', 'vigilance', 'concentration', 'individuals', 'engaged', 'complex', 'comprehension', 'activities', 'reading', 'understanding', 'one', 'recent', 'study', 'tracked', 'mind', 'wandering', 'individuals', 'countries', 'smartphone', 'app', 'prompted', 'people', 'thought', 'probes', 'random', 'intervals', 'throughout', 'day', 'people', 'reported', 'mind', 'wandering', 'prompts', 'confirmed', 'lab', 'studies', 'pervasiveness', 'mind', 'wandering', 'see', 'review', 'mind', 'wandering', 'merely', 'incidental', 'recent', 'meta', 'analysis', 'samples', 'indicated', 'negative', 'correlation', 'mind', 'wandering', 'performance', 'across', 'variety', 'tasks', 'correlation', 'increases', 'task', 'complexity', 'compounded', 'high', 'frequency', 'mind', 'wandering', 'serious', 'consequences', 'performance', 'productivity', 'society', 'large', 'learning', 'technology', 'traditional', 'learning', 'technologies', 'rely', 'assumption', 'students', 'attending', 'learning', 'session', 'although', 'always', 'case', 'example', 'estimated', 'students', 'mind', 'wander', 'approximately', 'time', 'engaging', 'online', 'lectures', 'important', 'component', 'moocs', 'advanced', 'technologies', 'aim', 'detect', 'respond', 'affective', 'states', 'like', 'boredom', 'evidence', 'effectiveness', 'still', 'equivocal', 'see', 'review', 'boredom', 'related', 'attention', 'technologies', 'aim', 'prevent', 'mind', 'wandering', 'engendering', 'highly', 'immersive', 'learning', 'experience', 'achieved', 'success', 'regard', 'done', 'attentional', 'focus', 'inevitably', 'wanes', 'session', 'progresses', 'novelty', 'system', 'content', 'fades', 'central', 'thesis', 'next', 'generation', 'learning', 'technologies', 'include', 'mechanisms', 'model', 'respond', 'learners', 'attention', 'real', 'time', 'attention', 'aware', 'technologies', 'model', 'various', 'aspects', 'learner', 'attention', 'e', 'g', 'divided', 'attention', 'alternating', 'attention', 'focus', 'detecting', 'mitigating', 'mind', 'wandering', 'quintessential', 'signal', 'waning', 'engagement', 'situate', 'work', 'context', 'reading', 'reading', 'common', 'activity', 'shared', 'across', 'multiple', 'learning', 'technologies', 'thereby', 'increasing', 'generalizability', 'results', 'students', 'mind', 'wander', 'approximately', 'time', 'computerized', 'reading', 'although', 'mind', 'wandering', 'facilitate', 'certain', 'cognitive', 'processes', 'like', 'future', 'planning', 'divergent', 'thinking', 'negatively', 'correlates', 'comprehension', 'learning', 'reviewed', 'suggesting', 'important', 'address', 'mind', 'wandering', 'learning', 'towards', 'end', 'developed', 'validated', 'closed', 'loop', 'attention', 'aware', 'learning', 'technology', 'combines', 'machinelearned', 'mind', 'wandering', 'detector', 'real', 'time', 'interpolated', 'testing', 're', 'study', 'intervention', 'attention', 'aware', 'technology', 'works', 'follows', 'learners', 'read', 'text', 'computer', 'screen', 'using', 'self', 'paced', 'screen', 'screen', 'also', 'called', 'page', 'page', 'reading', 'paradigm', 'track', 'eye', 'gaze', 'reading', 'using', 'remote', 'eye', 'tracker', 'restrict', 'head', 'movements', 'focus', 'eyegaze', 'mind', 'wandering', 'detection', 'due', 'decades', 'research', 'suggesting', 'tight', 'coupling', 'attentional', 'focus', 'eye', 'movements', 'reading', 'mind', 'wandering', 'detected', 'system', 'intervenes', 'attempt', 'redirect', 'attentional', 'focus', 'correct', 'comprehension', 'deficits', 'might', 'arise', 'due', 'mind', 'wandering', 'interventions', 'consist', 'asking', 'comprehension', 'question', 'pages', 'mind', 'wandering', 'detected', 'providing', 'opportunities', 're', 'read', 'based', 'learners', 'responses', 'paper', 'discuss', 'mind', 'wandering', 'mind', 'wandering', 'also', 'unfortunately', 'addressed', 'problem', 'education', 'yet', 'deeply', 'studied', 'context', 'detector', 'intervention', 'approach', 'results', 'summative', 'evaluation', 'study', 'related', 'work', 'idea', 'attention', 'aware', 'user', 'interfaces', 'new', 'proposed', 'almost', 'decade', 'ago', 'roda', 'thomas', 'even', 'article', 'futuristic', 'applications', 'attention', 'aware', 'systems', 'educational', 'contexts', 'prior', 'gluck', 'et', 'al', 'discussed', 'use', 'eye', 'tracking', 'increase', 'bandwidth', 'information', 'available', 'intelligent', 'tutoring', 'system', 'similarly', 'anderson', 'followed', 'ideas', 'demonstrating', 'particular', 'beneficial', 'instructional', 'strategies', 'could', 'launched', 'via', 'real', 'time', 'analysis', 'eye', 'gaze', 'recent', 'work', 'leveraging', 'eye', 'gaze', 'increase', 'bandwidth', 'learner', 'models', 'conati', 'et', 'al', 'provide', 'excellent', 'review', 'much', 'existing', 'work', 'area', 'group', 'research', 'three', 'categories', 'offline', 'analyses', 'eye', 'gaze', 'study', 'attentional', 'processes', 'computational', 'modeling', 'attentional', 'states', 'closed', 'loop', 'systems', 'respond', 'attention', 'real', 'time', 'offline', 'analysis', 'eye', 'movements', 'received', 'considerable', 'attention', 'cognitive', 'educational', 'psychology', 'several', 'decades', 'e', 'g', 'area', 'research', 'relatively', 'healthy', 'online', 'computational', 'models', 'learner', 'attention', 'beginning', 'emerge', 'e', 'g', 'closed', 'loop', 'attention', 'aware', 'systems', 'far', 'see', 'less', 'exhaustive', 'list', 'two', 'known', 'examples', 'gazetutor', 'attentivereview', 'discussed', 'gazetutor', 'learning', 'technology', 'biology', 'animated', 'conversational', 'agent', 'provides', 'spoken', 'explanations', 'biology', 'topics', 'synchronized', 'images', 'system', 'uses', 'tobii', 'eye', 'tracker', 'detect', 'inattention', 'assumed', 'occur', 'learners', 'gaze', 'tutor', 'agent', 'image', 'least', 'five', 'consecutive', 'seconds', 'occurs', 'system', 'interrupts', 'speech', 'mid', 'utterance', 'directs', 'learners', 'reorient', 'attention', 'e', 'g', 'm', 'know', 'repeats', 'speaking', 'start', 'current', 'utterance', 'evaluation', 'study', 'learners', 'undergraduate', 'students', 'completed', 'learning', 'session', 'four', 'biology', 'topics', 'attention', 'aware', 'components', 'enabled', 'experimental', 'group', 'disabled', 'control', 'group', 'results', 'indicated', 'gazetutor', 'successful', 'dynamically', 'reorienting', 'learners', 'attentional', 'patterns', 'towards', 'interface', 'importantly', 'learning', 'gains', 'deep', 'reasoning', 'questions', 'significantly', 'higher', 'experimental', 'vs', 'control', 'group', 'high', 'aptitude', 'learners', 'results', 'suggest', 'even', 'basic', 'attention', 'aware', 'technology', 'effective', 'improving', 'learning', 'least', 'subset', 'learners', 'however', 'key', 'limitation', 'researchers', 'simply', 'assumed', 'screen', 'gaze', 'corresponded', 'inattention', 'test', 'assumption', 'e', 'g', 'students', 'could', 'concentrating', 'eyes', 'closed', 'would', 'perceived', 'inattentive', 'attentivereview', 'closed', 'loop', 'system', 'mooc', 'learning', 'mobile', 'phones', 'system', 'uses', 'video', 'based', 'photoplethysmography', 'ppg', 'detect', 'learners', 'heart', 'rate', 'back', 'camera', 'smartphone', 'view', 'mooc', 'like', 'lectures', 'phone', 'attentivereview', 'ranks', 'lectures', 'based', 'estimates', 'learners', 'perceived', 'difficulty', 'selecting', 'difficult', 'lecture', 'subsequent', 'review', 'called', 'adaptive', 'review', 'participant', 'subjects', 'evaluation', 'study', 'authors', 'found', 'learning', 'gains', 'obtained', 'adaptive', 'review', 'condition', 'statistically', 'par', 'full', 'review', 'condition', 'achieved', 'less', 'review', 'time', 'although', 'result', 'suggests', 'attentivereview', 'increased', 'learning', 'efficiency', 'question', 'whether', 'system', 'even', 'considered', 'attention', 'aware', 'technology', 'arguable', 'system', 'anything', 'attention', 'except', 'attention', 'appearing', 'name', 'selects', 'items', 'review', 'based', 'model', 'perceived', 'difficulty', 'learners', 'attentional', 'state', 'two', 'might', 'related', 'clearly', 'novelty', 'paper', 'focuses', 'closing', 'loop', 'research', 'educational', 'data', 'learning', 'outcomes', 'developing', 'validating', 'first', 'view', 'real', 'time', 'learning', 'technology', 'detects', 'mitigates', 'mind', 'wandering', 'computerized', 'reading', 'although', 'automated', 'detection', 'complex', 'mental', 'states', 'goal', 'developing', 'intelligent', 'learning', 'technologies', 'respond', 'sensed', 'states', 'active', 'research', 'area', 'see', 'reviews', 'mind', 'wandering', 'rarely', 'explored', 'aspect', 'learner', 'mental', 'state', 'warrants', 'detection', 'corrective', 'action', 'work', 'modeling', 'locus', 'learner', 'attention', 'see', 'review', 'mind', 'wandering', 'inherently', 'different', 'commonly', 'studied', 'forms', 'attention', 'e', 'g', 'selective', 'attention', 'distraction', 'involves', 'covert', 'forms', 'involuntary', 'attentional', 'lapses', 'spawned', 'self', 'generated', 'internal', 'thought', 'simply', 'put', 'mind', 'wandering', 'form', 'looking', 'without', 'seeing', 'eyes', 'might', 'fixated', 'appropriate', 'external', 'stimulus', 'little', 'processed', 'mind', 'consumed', 'stimulusindependent', 'internal', 'thoughts', 'offline', 'automated', 'approaches', 'detect', 'mind', 'wandering', 'developed', 'e', 'g', 'detectors', 'yet', 'used', 'trigger', 'online', 'interventions', 'adapt']\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "nostop = []\n",
    "for i, doc in enumerate(docs):\n",
    "    doc = doc.split()\n",
    "    print(len(doc), end=' ')\n",
    "    nos=()\n",
    "    for word in list(doc):\n",
    "        if word in stop_words:\n",
    "            doc.remove(word)\n",
    "    nostop.append(doc)\n",
    "    print(len(doc))\n",
    "print(len(nostop))\n",
    "print(nostop[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3331\n",
      "3131\n",
      "3653\n",
      "2440\n",
      "3132\n",
      "3479\n",
      "2894\n",
      "2541\n",
      "3308\n",
      "3601\n",
      "3884\n",
      "3408\n",
      "2648\n",
      "3659\n",
      "2957\n",
      "3510\n",
      "3988\n",
      "17\n",
      "['abstract', 'mind', 'wandering', 'defined', 'shifts', 'attention', 'task', 'related', 'processing', 'task', 'unrelated', 'thoughts', 'ubiquitous', 'phenomenon', 'negative', 'influence', 'performance', 'productivity', 'many', 'contexts', 'including', 'learning', 'propose', 'next', 'generation', 'learning', 'technologies', 'mechanism', 'detect', 'respond', 'mind', 'wandering', 'real', 'time', 'towards', 'end', 'developed', 'technology', 'automatically', 'detects', 'mind', 'wandering', 'eye', 'gaze', 'learning', 'instructional', 'texts', 'mind', 'wandering', 'detected', 'technology', 'intervenes', 'posing', 'time', 'questions', 'encouraging', 'reading', 'needed', 'multiple', 'rounds', 'iterative', 'refinement', 'summatively', 'compared', 'technology', 'yoked', 'control', 'experiment', 'participants', 'key', 'dependent', 'variable', 'performance', 'post', 'reading', 'comprehension', 'assessment', 'results', 'suggest', 'technology', 'successful', 'correcting', 'comprehension', 'deficits', 'attributed', 'mind', 'wandering', 'sigma', 'specific', 'conditions', 'thereby', 'highlighting', 'potential', 'improve', 'learning', 'attending', 'attention', 'keywords', 'mind', 'wandering', 'gaze', 'tracking', 'student', 'modeling', 'attentionaware', 'introduction', 'despite', 'best', 'efforts', 'write', 'clear', 'engaging', 'paper', 'chances', 'high', 'within', 'next', 'pages', 'might', 'fall', 'prey', 'referred', 'zoning', 'daydreaming', 'mind', 'wandering', 'despite', 'best', 'intention', 'concentrate', 'paper', 'point', 'attention', 'might', 'drift', 'away', 'unrelated', 'thoughts', 'lunch', 'childcare', 'upcoming', 'trip', 'prediction', 'based', 'negative', 'cynical', 'opinion', 'reader', 'reviewer', 'read', 'review', 'papers', 'known', 'attentional', 'control', 'vigilance', 'concentration', 'individuals', 'engaged', 'complex', 'comprehension', 'activities', 'reading', 'understanding', 'one', 'recent', 'study', 'tracked', 'mind', 'wandering', 'individuals', 'countries', 'smartphone', 'app', 'prompted', 'people', 'thought', 'probes', 'random', 'intervals', 'throughout', 'day', 'people', 'reported', 'mind', 'wandering', 'prompts', 'confirmed', 'lab', 'studies', 'pervasiveness', 'mind', 'wandering', 'see', 'review', 'mind', 'wandering', 'merely', 'incidental', 'recent', 'meta', 'analysis', 'samples', 'indicated', 'negative', 'correlation', 'mind', 'wandering', 'performance', 'across', 'variety', 'tasks', 'correlation', 'increases', 'task', 'complexity', 'compounded', 'high', 'frequency', 'mind', 'wandering', 'serious', 'consequences', 'performance', 'productivity', 'society', 'large', 'learning', 'technology', 'traditional', 'learning', 'technologies', 'rely', 'assumption', 'students', 'attending', 'learning', 'session', 'although', 'always', 'case', 'example', 'estimated', 'students', 'mind', 'wander', 'approximately', 'time', 'engaging', 'online', 'lectures', 'important', 'component', 'moocs', 'advanced', 'technologies', 'aim', 'detect', 'respond', 'affective', 'states', 'like', 'boredom', 'evidence', 'effectiveness', 'still', 'equivocal', 'see', 'review', 'boredom', 'related', 'attention', 'technologies', 'aim', 'prevent', 'mind', 'wandering', 'engendering', 'highly', 'immersive', 'learning', 'experience', 'achieved', 'success', 'regard', 'done', 'attentional', 'focus', 'inevitably', 'wanes', 'session', 'progresses', 'novelty', 'system', 'content', 'fades', 'central', 'thesis', 'next', 'generation', 'learning', 'technologies', 'include', 'mechanisms', 'model', 'respond', 'learners', 'attention', 'real', 'time', 'attention', 'aware', 'technologies', 'model', 'various', 'aspects', 'learner', 'attention', 'divided', 'attention', 'alternating', 'attention', 'focus', 'detecting', 'mitigating', 'mind', 'wandering', 'quintessential', 'signal', 'waning', 'engagement', 'situate', 'work', 'context', 'reading', 'reading', 'common', 'activity', 'shared', 'across', 'multiple', 'learning', 'technologies', 'thereby', 'increasing', 'generalizability', 'results', 'students', 'mind', 'wander', 'approximately', 'time', 'computerized', 'reading', 'although', 'mind', 'wandering', 'facilitate', 'certain', 'cognitive', 'processes', 'like', 'future', 'planning', 'divergent', 'thinking', 'negatively', 'correlates', 'comprehension', 'learning', 'reviewed', 'suggesting', 'important', 'address', 'mind', 'wandering', 'learning', 'towards', 'end', 'developed', 'validated', 'closed', 'loop', 'attention', 'aware', 'learning', 'technology', 'combines', 'machinelearned', 'mind', 'wandering', 'detector', 'real', 'time', 'interpolated', 'testing', 'study', 'intervention', 'attention', 'aware', 'technology', 'works', 'follows', 'learners', 'read', 'text', 'computer', 'screen', 'using', 'self', 'paced', 'screen', 'screen', 'also', 'called', 'page', 'page', 'reading', 'paradigm', 'track', 'eye', 'gaze', 'reading', 'using', 'remote', 'eye', 'tracker', 'restrict', 'head', 'movements', 'focus', 'eyegaze', 'mind', 'wandering', 'detection', 'due', 'decades', 'research', 'suggesting', 'tight', 'coupling', 'attentional', 'focus', 'eye', 'movements', 'reading', 'mind', 'wandering', 'detected', 'system', 'intervenes', 'attempt', 'redirect', 'attentional', 'focus', 'correct', 'comprehension', 'deficits', 'might', 'arise', 'due', 'mind', 'wandering', 'interventions', 'consist', 'asking', 'comprehension', 'question', 'pages', 'mind', 'wandering', 'detected', 'providing', 'opportunities', 'read', 'based', 'learners', 'responses', 'paper', 'discuss', 'mind', 'wandering', 'mind', 'wandering', 'also', 'unfortunately', 'addressed', 'problem', 'education', 'yet', 'deeply', 'studied', 'context', 'detector', 'intervention', 'approach', 'results', 'summative', 'evaluation', 'study', 'related', 'work', 'idea', 'attention', 'aware', 'user', 'interfaces', 'new', 'proposed', 'almost', 'decade', 'ago', 'roda', 'thomas', 'even', 'article', 'futuristic', 'applications', 'attention', 'aware', 'systems', 'educational', 'contexts', 'prior', 'gluck', 'discussed', 'use', 'eye', 'tracking', 'increase', 'bandwidth', 'information', 'available', 'intelligent', 'tutoring', 'system', 'similarly', 'anderson', 'followed', 'ideas', 'demonstrating', 'particular', 'beneficial', 'instructional', 'strategies', 'could', 'launched', 'via', 'real', 'time', 'analysis', 'eye', 'gaze', 'recent', 'work', 'leveraging', 'eye', 'gaze', 'increase', 'bandwidth', 'learner', 'models', 'conati', 'provide', 'excellent', 'review', 'much', 'existing', 'work', 'area', 'group', 'research', 'three', 'categories', 'offline', 'analyses', 'eye', 'gaze', 'study', 'attentional', 'processes', 'computational', 'modeling', 'attentional', 'states', 'closed', 'loop', 'systems', 'respond', 'attention', 'real', 'time', 'offline', 'analysis', 'eye', 'movements', 'received', 'considerable', 'attention', 'cognitive', 'educational', 'psychology', 'several', 'decades', 'area', 'research', 'relatively', 'healthy', 'online', 'computational', 'models', 'learner', 'attention', 'beginning', 'emerge', 'closed', 'loop', 'attention', 'aware', 'systems', 'far', 'see', 'less', 'exhaustive', 'list', 'two', 'known', 'examples', 'gazetutor', 'attentivereview', 'discussed', 'gazetutor', 'learning', 'technology', 'biology', 'animated', 'conversational', 'agent', 'provides', 'spoken', 'explanations', 'biology', 'topics', 'synchronized', 'images', 'system', 'uses', 'tobii', 'eye', 'tracker', 'detect', 'inattention', 'assumed', 'occur', 'learners', 'gaze', 'tutor', 'agent', 'image', 'least', 'five', 'consecutive', 'seconds', 'occurs', 'system', 'interrupts', 'speech', 'mid', 'utterance', 'directs', 'learners', 'reorient', 'attention', 'know', 'repeats', 'speaking', 'start', 'current', 'utterance', 'evaluation', 'study', 'learners', 'undergraduate', 'students', 'completed', 'learning', 'session', 'four', 'biology', 'topics', 'attention', 'aware', 'components', 'enabled', 'experimental', 'group', 'disabled', 'control', 'group', 'results', 'indicated', 'gazetutor', 'successful', 'dynamically', 'reorienting', 'learners', 'attentional', 'patterns', 'towards', 'interface', 'importantly', 'learning', 'gains', 'deep', 'reasoning', 'questions', 'significantly', 'higher', 'experimental', 'control', 'group', 'high', 'aptitude', 'learners', 'results', 'suggest', 'even', 'basic', 'attention', 'aware', 'technology', 'effective', 'improving', 'learning', 'least', 'subset', 'learners', 'however', 'key', 'limitation', 'researchers', 'simply', 'assumed', 'screen', 'gaze', 'corresponded', 'inattention', 'test', 'assumption', 'students', 'could', 'concentrating', 'eyes', 'closed', 'would', 'perceived', 'inattentive', 'attentivereview', 'closed', 'loop', 'system', 'mooc', 'learning', 'mobile', 'phones', 'system', 'uses', 'video', 'based', 'photoplethysmography', 'ppg', 'detect', 'learners', 'heart', 'rate', 'back', 'camera', 'smartphone', 'view', 'mooc', 'like', 'lectures', 'phone', 'attentivereview', 'ranks', 'lectures', 'based', 'estimates', 'learners', 'perceived', 'difficulty', 'selecting', 'difficult', 'lecture', 'subsequent', 'review', 'called', 'adaptive', 'review', 'participant', 'subjects', 'evaluation', 'study', 'authors', 'found', 'learning', 'gains', 'obtained', 'adaptive', 'review', 'condition', 'statistically', 'par', 'full', 'review', 'condition', 'achieved', 'less', 'review', 'time', 'although', 'result', 'suggests', 'attentivereview', 'increased', 'learning', 'efficiency', 'question', 'whether', 'system', 'even', 'considered', 'attention', 'aware', 'technology', 'arguable', 'system', 'anything', 'attention', 'except', 'attention', 'appearing', 'name', 'selects', 'items', 'review', 'based', 'model', 'perceived', 'difficulty', 'learners', 'attentional', 'state', 'two', 'might', 'related', 'clearly', 'novelty', 'paper', 'focuses', 'closing', 'loop', 'research', 'educational', 'data', 'learning', 'outcomes', 'developing', 'validating', 'first', 'view', 'real', 'time', 'learning', 'technology', 'detects', 'mitigates', 'mind', 'wandering', 'computerized', 'reading', 'although', 'automated', 'detection', 'complex', 'mental', 'states', 'goal', 'developing', 'intelligent', 'learning', 'technologies', 'respond', 'sensed', 'states', 'active', 'research', 'area', 'see', 'reviews', 'mind', 'wandering', 'rarely', 'explored', 'aspect', 'learner', 'mental', 'state', 'warrants', 'detection', 'corrective', 'action', 'work', 'modeling', 'locus', 'learner', 'attention', 'see', 'review', 'mind', 'wandering', 'inherently', 'different', 'commonly', 'studied', 'forms', 'attention', 'selective', 'attention', 'distraction', 'involves', 'covert', 'forms', 'involuntary', 'attentional', 'lapses', 'spawned', 'self', 'generated', 'internal', 'thought', 'simply', 'put', 'mind', 'wandering', 'form', 'looking', 'without', 'seeing', 'eyes', 'might', 'fixated', 'appropriate', 'external', 'stimulus', 'little', 'processed', 'mind', 'consumed', 'stimulusindependent', 'internal', 'thoughts', 'offline', 'automated', 'approaches', 'detect', 'mind', 'wandering', 'developed', 'detectors', 'yet', 'used', 'trigger', 'online', 'interventions', 'adapt', 'offline', 'gaze', 'based', 'automated', 'mind', 'wandering', 'detector', 'trigger', 'real', 'time', 'interventions', 'address', 'mind', 'wandering', 'reading', 'conduct', 'randomized', 'control', 'trial', 'evaluate', 'efficacy', 'attentionaware', 'learning', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "cleaned = []\n",
    "for i, doc in enumerate(nostop):\n",
    "    for word in list(doc):\n",
    "        if len(word) <= 2:\n",
    "            doc.remove(word)\n",
    "    cleaned.append(doc)\n",
    "    print(len(doc))\n",
    "print(len(cleaned))\n",
    "print(cleaned[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    \n",
    "    cleaned_docs = []\n",
    "\n",
    "    docs = []\n",
    "    for document in documents:\n",
    "        abstract = document.index('abstract', 0, 8000)\n",
    "        reference = document.rfind('reference')\n",
    "        newtext = document[abstract:reference]\n",
    "        docs.append(newtext)\n",
    "        \n",
    "    for i, doc in enumerate(docs):\n",
    "        docs[i] = doc.replace('\\n', ' ')\n",
    "\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "    numbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        for punc in punctuation:\n",
    "            docs[i]=doc.replace(punc, ' ')\n",
    "            doc = docs[i]\n",
    "            \n",
    "    for i, doc in enumerate(docs):\n",
    "        for num in numbers:\n",
    "            docs[i]=doc.replace(num, ' ')\n",
    "            doc = docs[i]\n",
    "\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "    \n",
    "    nostop = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc = doc.split()\n",
    "        nos=()\n",
    "        for word in list(doc):\n",
    "            if word in stop_words:\n",
    "                doc.remove(word)\n",
    "        nostop.append(doc)\n",
    "    \n",
    "    for i, doc in enumerate(nostop):\n",
    "        for word in list(doc):\n",
    "            if len(word) <= 2:\n",
    "                doc.remove(word)\n",
    "        cleaned_docs.append(doc)\n",
    "\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['abstract', 'mind', 'wandering', 'defined', 'shifts', 'attention', 'task', 'related', 'processing', 'task', 'unrelated', 'thoughts', 'ubiquitous', 'phenomenon', 'negative', 'influence', 'performance', 'productivity', 'many', 'contexts', 'including', 'learning', 'propose', 'next', 'generation', 'learning', 'technologies', 'mechanism', 'detect', 'respond', 'mind', 'wandering', 'real', 'time', 'towards', 'end', 'developed', 'technology', 'automatically', 'detects', 'mind', 'wandering', 'eye', 'gaze', 'learning', 'instructional', 'texts', 'mind', 'wandering', 'detected', 'technology', 'intervenes', 'posing', 'time', 'questions', 'encouraging', 'reading', 'needed', 'multiple', 'rounds', 'iterative', 'refinement', 'summatively', 'compared', 'technology', 'yoked', 'control', 'experiment', 'participants', 'key', 'dependent', 'variable', 'performance', 'post', 'reading', 'comprehension', 'assessment', 'results', 'suggest', 'technology', 'successful', 'correcting', 'comprehension', 'deficits', 'attributed', 'mind', 'wandering', 'sigma', 'specific', 'conditions', 'thereby', 'highlighting', 'potential', 'improve', 'learning', 'attending', 'attention', 'keywords', 'mind', 'wandering', 'gaze', 'tracking', 'student', 'modeling', 'attentionaware', 'introduction', 'despite', 'best', 'efforts', 'write', 'clear', 'engaging', 'paper', 'chances', 'high', 'within', 'next', 'pages', 'might', 'fall', 'prey', 'referred', 'zoning', 'daydreaming', 'mind', 'wandering', 'despite', 'best', 'intention', 'concentrate', 'paper', 'point', 'attention', 'might', 'drift', 'away', 'unrelated', 'thoughts', 'lunch', 'childcare', 'upcoming', 'trip', 'prediction', 'based', 'negative', 'cynical', 'opinion', 'reader', 'reviewer', 'read', 'review', 'papers', 'known', 'attentional', 'control', 'vigilance', 'concentration', 'individuals', 'engaged', 'complex', 'comprehension', 'activities', 'reading', 'understanding', 'one', 'recent', 'study', 'tracked', 'mind', 'wandering', 'individuals', 'countries', 'smartphone', 'app', 'prompted', 'people', 'thought', 'probes', 'random', 'intervals', 'throughout', 'day', 'people', 'reported', 'mind', 'wandering', 'prompts', 'confirmed', 'lab', 'studies', 'pervasiveness', 'mind', 'wandering', 'see', 'review', 'mind', 'wandering', 'merely', 'incidental', 'recent', 'meta', 'analysis', 'samples', 'indicated', 'negative', 'correlation', 'mind', 'wandering', 'performance', 'across', 'variety', 'tasks', 'correlation', 'increases', 'task', 'complexity', 'compounded', 'high', 'frequency', 'mind', 'wandering', 'serious', 'consequences', 'performance', 'productivity', 'society', 'large', 'learning', 'technology', 'traditional', 'learning', 'technologies', 'rely', 'assumption', 'students', 'attending', 'learning', 'session', 'although', 'always', 'case', 'example', 'estimated', 'students', 'mind', 'wander', 'approximately', 'time', 'engaging', 'online', 'lectures', 'important', 'component', 'moocs', 'advanced', 'technologies', 'aim', 'detect', 'respond', 'affective', 'states', 'like', 'boredom', 'evidence', 'effectiveness', 'still', 'equivocal', 'see', 'review', 'boredom', 'related', 'attention', 'technologies', 'aim', 'prevent', 'mind', 'wandering', 'engendering', 'highly', 'immersive', 'learning', 'experience', 'achieved', 'success', 'regard', 'done', 'attentional', 'focus', 'inevitably', 'wanes', 'session', 'progresses', 'novelty', 'system', 'content', 'fades', 'central', 'thesis', 'next', 'generation', 'learning', 'technologies', 'include', 'mechanisms', 'model', 'respond', 'learners', 'attention', 'real', 'time', 'attention', 'aware', 'technologies', 'model', 'various', 'aspects', 'learner', 'attention', 'divided', 'attention', 'alternating', 'attention', 'focus', 'detecting', 'mitigating', 'mind', 'wandering', 'quintessential', 'signal', 'waning', 'engagement', 'situate', 'work', 'context', 'reading', 'reading', 'common', 'activity', 'shared', 'across', 'multiple', 'learning', 'technologies', 'thereby', 'increasing', 'generalizability', 'results', 'students', 'mind', 'wander', 'approximately', 'time', 'computerized', 'reading', 'although', 'mind', 'wandering', 'facilitate', 'certain', 'cognitive', 'processes', 'like', 'future', 'planning', 'divergent', 'thinking', 'negatively', 'correlates', 'comprehension', 'learning', 'reviewed', 'suggesting', 'important', 'address', 'mind', 'wandering', 'learning', 'towards', 'end', 'developed', 'validated', 'closed', 'loop', 'attention', 'aware', 'learning', 'technology', 'combines', 'machinelearned', 'mind', 'wandering', 'detector', 'real', 'time', 'interpolated', 'testing', 'study', 'intervention', 'attention', 'aware', 'technology', 'works', 'follows', 'learners', 'read', 'text', 'computer', 'screen', 'using', 'self', 'paced', 'screen', 'screen', 'also', 'called', 'page', 'page', 'reading', 'paradigm', 'track', 'eye', 'gaze', 'reading', 'using', 'remote', 'eye', 'tracker', 'restrict', 'head', 'movements', 'focus', 'eyegaze', 'mind', 'wandering', 'detection', 'due', 'decades', 'research', 'suggesting', 'tight', 'coupling', 'attentional', 'focus', 'eye', 'movements', 'reading', 'mind', 'wandering', 'detected', 'system', 'intervenes', 'attempt', 'redirect', 'attentional', 'focus', 'correct', 'comprehension', 'deficits', 'might', 'arise', 'due', 'mind', 'wandering', 'interventions', 'consist', 'asking', 'comprehension', 'question', 'pages', 'mind', 'wandering', 'detected', 'providing', 'opportunities', 'read', 'based', 'learners', 'responses', 'paper', 'discuss', 'mind', 'wandering', 'mind', 'wandering', 'also', 'unfortunately', 'addressed', 'problem', 'education', 'yet', 'deeply', 'studied', 'context', 'detector', 'intervention', 'approach', 'results', 'summative', 'evaluation', 'study', 'related', 'work', 'idea', 'attention', 'aware', 'user', 'interfaces', 'new', 'proposed', 'almost', 'decade', 'ago', 'roda', 'thomas', 'even', 'article', 'futuristic', 'applications', 'attention', 'aware', 'systems', 'educational', 'contexts', 'prior', 'gluck', 'discussed', 'use', 'eye', 'tracking', 'increase', 'bandwidth', 'information', 'available', 'intelligent', 'tutoring', 'system', 'similarly', 'anderson', 'followed', 'ideas', 'demonstrating', 'particular', 'beneficial', 'instructional', 'strategies', 'could', 'launched', 'via', 'real', 'time', 'analysis', 'eye', 'gaze', 'recent', 'work', 'leveraging', 'eye', 'gaze', 'increase', 'bandwidth', 'learner', 'models', 'conati', 'provide', 'excellent', 'review', 'much', 'existing', 'work', 'area', 'group', 'research', 'three', 'categories', 'offline', 'analyses', 'eye', 'gaze', 'study', 'attentional', 'processes', 'computational', 'modeling', 'attentional', 'states', 'closed', 'loop', 'systems', 'respond', 'attention', 'real', 'time', 'offline', 'analysis', 'eye', 'movements', 'received', 'considerable', 'attention', 'cognitive', 'educational', 'psychology', 'several', 'decades', 'area', 'research', 'relatively', 'healthy', 'online', 'computational', 'models', 'learner', 'attention', 'beginning', 'emerge', 'closed', 'loop', 'attention', 'aware', 'systems', 'far', 'see', 'less', 'exhaustive', 'list', 'two', 'known', 'examples', 'gazetutor', 'attentivereview', 'discussed', 'gazetutor', 'learning', 'technology', 'biology', 'animated', 'conversational', 'agent', 'provides', 'spoken', 'explanations', 'biology', 'topics', 'synchronized', 'images', 'system', 'uses', 'tobii', 'eye', 'tracker', 'detect', 'inattention', 'assumed', 'occur', 'learners', 'gaze', 'tutor', 'agent', 'image', 'least', 'five', 'consecutive', 'seconds', 'occurs', 'system', 'interrupts', 'speech', 'mid', 'utterance', 'directs', 'learners', 'reorient', 'attention', 'know', 'repeats', 'speaking', 'start', 'current', 'utterance', 'evaluation', 'study', 'learners', 'undergraduate', 'students', 'completed', 'learning', 'session', 'four', 'biology', 'topics', 'attention', 'aware', 'components', 'enabled', 'experimental', 'group', 'disabled', 'control', 'group', 'results', 'indicated', 'gazetutor', 'successful', 'dynamically', 'reorienting', 'learners', 'attentional', 'patterns', 'towards', 'interface', 'importantly', 'learning', 'gains', 'deep', 'reasoning', 'questions', 'significantly', 'higher', 'experimental', 'control', 'group', 'high', 'aptitude', 'learners', 'results', 'suggest', 'even', 'basic', 'attention', 'aware', 'technology', 'effective', 'improving', 'learning', 'least', 'subset', 'learners', 'however', 'key', 'limitation', 'researchers', 'simply', 'assumed', 'screen', 'gaze', 'corresponded', 'inattention', 'test', 'assumption', 'students', 'could', 'concentrating', 'eyes', 'closed', 'would', 'perceived', 'inattentive', 'attentivereview', 'closed', 'loop', 'system', 'mooc', 'learning', 'mobile', 'phones', 'system', 'uses', 'video', 'based', 'photoplethysmography', 'ppg', 'detect', 'learners', 'heart', 'rate', 'back', 'camera', 'smartphone', 'view', 'mooc', 'like', 'lectures', 'phone', 'attentivereview', 'ranks', 'lectures', 'based', 'estimates', 'learners', 'perceived', 'difficulty', 'selecting', 'difficult', 'lecture', 'subsequent', 'review', 'called', 'adaptive', 'review', 'participant', 'subjects', 'evaluation', 'study', 'authors', 'found', 'learning', 'gains', 'obtained', 'adaptive', 'review', 'condition', 'statistically', 'par', 'full', 'review', 'condition', 'achieved', 'less', 'review', 'time', 'although', 'result', 'suggests', 'attentivereview', 'increased', 'learning', 'efficiency', 'question', 'whether', 'system', 'even', 'considered', 'attention', 'aware', 'technology', 'arguable', 'system', 'anything', 'attention', 'except', 'attention', 'appearing', 'name', 'selects', 'items', 'review', 'based', 'model', 'perceived', 'difficulty', 'learners', 'attentional', 'state', 'two', 'might', 'related', 'clearly', 'novelty', 'paper', 'focuses', 'closing', 'loop', 'research', 'educational', 'data', 'learning', 'outcomes', 'developing', 'validating', 'first', 'view', 'real', 'time', 'learning', 'technology', 'detects', 'mitigates', 'mind', 'wandering', 'computerized', 'reading', 'although', 'automated', 'detection', 'complex', 'mental', 'states', 'goal', 'developing', 'intelligent', 'learning', 'technologies', 'respond', 'sensed', 'states', 'active', 'research', 'area', 'see', 'reviews', 'mind', 'wandering', 'rarely', 'explored', 'aspect', 'learner', 'mental', 'state', 'warrants', 'detection', 'corrective', 'action', 'work', 'modeling', 'locus', 'learner', 'attention', 'see', 'review', 'mind', 'wandering', 'inherently', 'different', 'commonly', 'studied', 'forms', 'attention', 'selective', 'attention', 'distraction', 'involves', 'covert', 'forms', 'involuntary', 'attentional', 'lapses', 'spawned', 'self', 'generated', 'internal', 'thought', 'simply', 'put', 'mind', 'wandering', 'form', 'looking', 'without', 'seeing', 'eyes', 'might', 'fixated', 'appropriate', 'external', 'stimulus', 'little', 'processed', 'mind', 'consumed', 'stimulusindependent', 'internal', 'thoughts', 'offline', 'automated', 'approaches', 'detect', 'mind', 'wandering', 'developed', 'detectors', 'yet', 'used', 'trigger', 'online', 'interventions', 'adapt', 'offline', 'gaze', 'based', 'automated', 'mind', 'wandering', 'detector', 'trigger', 'real', 'time', 'interventions', 'address', 'mind', 'wandering', 'reading', 'conduct', 'randomized', 'control', 'trial', 'evaluate', 'efficacy', 'attentionaware', 'learning', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "for text in text_file:\n",
    "    file= open(text,'r',encoding='utf-8')\n",
    "    documents.append(file.read())\n",
    "print(len(documents))\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "cleaneddoc = clean_list_of_documents(documents)\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "print(cleaneddoc[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): \n",
    "\n",
    "First, the vocabulary is pruned using a list of stopwords, so it does not count all stopwords that cannot convey the meaning of the text\n",
    "\n",
    "Second, figuring out the vocabulary allows us to map the text into vectors which present the similarity of the meaning of different passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5666\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        uniquewords = set(doc)\n",
    "        uniquewords = sorted(uniquewords)\n",
    "        voc.append(uniquewords)\n",
    "        \n",
    "    allvoc = sum(voc, [])\n",
    "    uniquewords = set(allvoc)\n",
    "    voc = sorted(uniquewords)\n",
    "        \n",
    "    \n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "\n",
    "lenclean = get_vocabulary(cleaneddoc)\n",
    "print(len(lenclean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n",
    "# the size of Sherin's vocabulary is 1429 and pruned vocabulary is 647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "def flatten_and_overlap(documents):\n",
    "    \n",
    "    wordschunk = []\n",
    "    \n",
    "    allvoc = sum(documents, [])\n",
    "    len(allvoc)\n",
    "    alist = []\n",
    "    count = 0\n",
    "    numchunk = 1\n",
    "    while (count < len(allvoc)):\n",
    "        if len(alist) <= 99:\n",
    "            alist.append(allvoc[count])\n",
    "            count += 1\n",
    "        else:\n",
    "            wordschunk.append(alist)\n",
    "            print('The number of words in chunk ' + str(numchunk) + \" is \" + str(len(alist)))\n",
    "            numchunk +=1\n",
    "            alist = []\n",
    "            count = count - 25\n",
    "    print('There are ' + str(len(wordschunck)) + ' chunks in total')\n",
    "    return wordschunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in chunk 1 is 100\n",
      "The number of words in chunk 2 is 100\n",
      "The number of words in chunk 3 is 100\n",
      "The number of words in chunk 4 is 100\n",
      "The number of words in chunk 5 is 100\n",
      "The number of words in chunk 6 is 100\n",
      "The number of words in chunk 7 is 100\n",
      "The number of words in chunk 8 is 100\n",
      "The number of words in chunk 9 is 100\n",
      "The number of words in chunk 10 is 100\n",
      "The number of words in chunk 11 is 100\n",
      "The number of words in chunk 12 is 100\n",
      "The number of words in chunk 13 is 100\n",
      "The number of words in chunk 14 is 100\n",
      "The number of words in chunk 15 is 100\n",
      "The number of words in chunk 16 is 100\n",
      "The number of words in chunk 17 is 100\n",
      "The number of words in chunk 18 is 100\n",
      "The number of words in chunk 19 is 100\n",
      "The number of words in chunk 20 is 100\n",
      "The number of words in chunk 21 is 100\n",
      "The number of words in chunk 22 is 100\n",
      "The number of words in chunk 23 is 100\n",
      "The number of words in chunk 24 is 100\n",
      "The number of words in chunk 25 is 100\n",
      "The number of words in chunk 26 is 100\n",
      "The number of words in chunk 27 is 100\n",
      "The number of words in chunk 28 is 100\n",
      "The number of words in chunk 29 is 100\n",
      "The number of words in chunk 30 is 100\n",
      "The number of words in chunk 31 is 100\n",
      "The number of words in chunk 32 is 100\n",
      "The number of words in chunk 33 is 100\n",
      "The number of words in chunk 34 is 100\n",
      "The number of words in chunk 35 is 100\n",
      "The number of words in chunk 36 is 100\n",
      "The number of words in chunk 37 is 100\n",
      "The number of words in chunk 38 is 100\n",
      "The number of words in chunk 39 is 100\n",
      "The number of words in chunk 40 is 100\n",
      "The number of words in chunk 41 is 100\n",
      "The number of words in chunk 42 is 100\n",
      "The number of words in chunk 43 is 100\n",
      "The number of words in chunk 44 is 100\n",
      "The number of words in chunk 45 is 100\n",
      "The number of words in chunk 46 is 100\n",
      "The number of words in chunk 47 is 100\n",
      "The number of words in chunk 48 is 100\n",
      "The number of words in chunk 49 is 100\n",
      "The number of words in chunk 50 is 100\n",
      "The number of words in chunk 51 is 100\n",
      "The number of words in chunk 52 is 100\n",
      "The number of words in chunk 53 is 100\n",
      "The number of words in chunk 54 is 100\n",
      "The number of words in chunk 55 is 100\n",
      "The number of words in chunk 56 is 100\n",
      "The number of words in chunk 57 is 100\n",
      "The number of words in chunk 58 is 100\n",
      "The number of words in chunk 59 is 100\n",
      "The number of words in chunk 60 is 100\n",
      "The number of words in chunk 61 is 100\n",
      "The number of words in chunk 62 is 100\n",
      "The number of words in chunk 63 is 100\n",
      "The number of words in chunk 64 is 100\n",
      "The number of words in chunk 65 is 100\n",
      "The number of words in chunk 66 is 100\n",
      "The number of words in chunk 67 is 100\n",
      "The number of words in chunk 68 is 100\n",
      "The number of words in chunk 69 is 100\n",
      "The number of words in chunk 70 is 100\n",
      "The number of words in chunk 71 is 100\n",
      "The number of words in chunk 72 is 100\n",
      "The number of words in chunk 73 is 100\n",
      "The number of words in chunk 74 is 100\n",
      "The number of words in chunk 75 is 100\n",
      "The number of words in chunk 76 is 100\n",
      "The number of words in chunk 77 is 100\n",
      "The number of words in chunk 78 is 100\n",
      "The number of words in chunk 79 is 100\n",
      "The number of words in chunk 80 is 100\n",
      "The number of words in chunk 81 is 100\n",
      "The number of words in chunk 82 is 100\n",
      "The number of words in chunk 83 is 100\n",
      "The number of words in chunk 84 is 100\n",
      "The number of words in chunk 85 is 100\n",
      "The number of words in chunk 86 is 100\n",
      "The number of words in chunk 87 is 100\n",
      "The number of words in chunk 88 is 100\n",
      "The number of words in chunk 89 is 100\n",
      "The number of words in chunk 90 is 100\n",
      "The number of words in chunk 91 is 100\n",
      "The number of words in chunk 92 is 100\n",
      "The number of words in chunk 93 is 100\n",
      "The number of words in chunk 94 is 100\n",
      "The number of words in chunk 95 is 100\n",
      "The number of words in chunk 96 is 100\n",
      "The number of words in chunk 97 is 100\n",
      "The number of words in chunk 98 is 100\n",
      "The number of words in chunk 99 is 100\n",
      "The number of words in chunk 100 is 100\n",
      "The number of words in chunk 101 is 100\n",
      "The number of words in chunk 102 is 100\n",
      "The number of words in chunk 103 is 100\n",
      "The number of words in chunk 104 is 100\n",
      "The number of words in chunk 105 is 100\n",
      "The number of words in chunk 106 is 100\n",
      "The number of words in chunk 107 is 100\n",
      "The number of words in chunk 108 is 100\n",
      "The number of words in chunk 109 is 100\n",
      "The number of words in chunk 110 is 100\n",
      "The number of words in chunk 111 is 100\n",
      "The number of words in chunk 112 is 100\n",
      "The number of words in chunk 113 is 100\n",
      "The number of words in chunk 114 is 100\n",
      "The number of words in chunk 115 is 100\n",
      "The number of words in chunk 116 is 100\n",
      "The number of words in chunk 117 is 100\n",
      "The number of words in chunk 118 is 100\n",
      "The number of words in chunk 119 is 100\n",
      "The number of words in chunk 120 is 100\n",
      "The number of words in chunk 121 is 100\n",
      "The number of words in chunk 122 is 100\n",
      "The number of words in chunk 123 is 100\n",
      "The number of words in chunk 124 is 100\n",
      "The number of words in chunk 125 is 100\n",
      "The number of words in chunk 126 is 100\n",
      "The number of words in chunk 127 is 100\n",
      "The number of words in chunk 128 is 100\n",
      "The number of words in chunk 129 is 100\n",
      "The number of words in chunk 130 is 100\n",
      "The number of words in chunk 131 is 100\n",
      "The number of words in chunk 132 is 100\n",
      "The number of words in chunk 133 is 100\n",
      "The number of words in chunk 134 is 100\n",
      "The number of words in chunk 135 is 100\n",
      "The number of words in chunk 136 is 100\n",
      "The number of words in chunk 137 is 100\n",
      "The number of words in chunk 138 is 100\n",
      "The number of words in chunk 139 is 100\n",
      "The number of words in chunk 140 is 100\n",
      "The number of words in chunk 141 is 100\n",
      "The number of words in chunk 142 is 100\n",
      "The number of words in chunk 143 is 100\n",
      "The number of words in chunk 144 is 100\n",
      "The number of words in chunk 145 is 100\n",
      "The number of words in chunk 146 is 100\n",
      "The number of words in chunk 147 is 100\n",
      "The number of words in chunk 148 is 100\n",
      "The number of words in chunk 149 is 100\n",
      "The number of words in chunk 150 is 100\n",
      "The number of words in chunk 151 is 100\n",
      "The number of words in chunk 152 is 100\n",
      "The number of words in chunk 153 is 100\n",
      "The number of words in chunk 154 is 100\n",
      "The number of words in chunk 155 is 100\n",
      "The number of words in chunk 156 is 100\n",
      "The number of words in chunk 157 is 100\n",
      "The number of words in chunk 158 is 100\n",
      "The number of words in chunk 159 is 100\n",
      "The number of words in chunk 160 is 100\n",
      "The number of words in chunk 161 is 100\n",
      "The number of words in chunk 162 is 100\n",
      "The number of words in chunk 163 is 100\n",
      "The number of words in chunk 164 is 100\n",
      "The number of words in chunk 165 is 100\n",
      "The number of words in chunk 166 is 100\n",
      "The number of words in chunk 167 is 100\n",
      "The number of words in chunk 168 is 100\n",
      "The number of words in chunk 169 is 100\n",
      "The number of words in chunk 170 is 100\n",
      "The number of words in chunk 171 is 100\n",
      "The number of words in chunk 172 is 100\n",
      "The number of words in chunk 173 is 100\n",
      "The number of words in chunk 174 is 100\n",
      "The number of words in chunk 175 is 100\n",
      "The number of words in chunk 176 is 100\n",
      "The number of words in chunk 177 is 100\n",
      "The number of words in chunk 178 is 100\n",
      "The number of words in chunk 179 is 100\n",
      "The number of words in chunk 180 is 100\n",
      "The number of words in chunk 181 is 100\n",
      "The number of words in chunk 182 is 100\n",
      "The number of words in chunk 183 is 100\n",
      "The number of words in chunk 184 is 100\n",
      "The number of words in chunk 185 is 100\n",
      "The number of words in chunk 186 is 100\n",
      "The number of words in chunk 187 is 100\n",
      "The number of words in chunk 188 is 100\n",
      "The number of words in chunk 189 is 100\n",
      "The number of words in chunk 190 is 100\n",
      "The number of words in chunk 191 is 100\n",
      "The number of words in chunk 192 is 100\n",
      "The number of words in chunk 193 is 100\n",
      "The number of words in chunk 194 is 100\n",
      "The number of words in chunk 195 is 100\n",
      "The number of words in chunk 196 is 100\n",
      "The number of words in chunk 197 is 100\n",
      "The number of words in chunk 198 is 100\n",
      "The number of words in chunk 199 is 100\n",
      "The number of words in chunk 200 is 100\n",
      "The number of words in chunk 201 is 100\n",
      "The number of words in chunk 202 is 100\n",
      "The number of words in chunk 203 is 100\n",
      "The number of words in chunk 204 is 100\n",
      "The number of words in chunk 205 is 100\n",
      "The number of words in chunk 206 is 100\n",
      "The number of words in chunk 207 is 100\n",
      "The number of words in chunk 208 is 100\n",
      "The number of words in chunk 209 is 100\n",
      "The number of words in chunk 210 is 100\n",
      "The number of words in chunk 211 is 100\n",
      "The number of words in chunk 212 is 100\n",
      "The number of words in chunk 213 is 100\n",
      "The number of words in chunk 214 is 100\n",
      "The number of words in chunk 215 is 100\n",
      "The number of words in chunk 216 is 100\n",
      "The number of words in chunk 217 is 100\n",
      "The number of words in chunk 218 is 100\n",
      "The number of words in chunk 219 is 100\n",
      "The number of words in chunk 220 is 100\n",
      "The number of words in chunk 221 is 100\n",
      "The number of words in chunk 222 is 100\n",
      "The number of words in chunk 223 is 100\n",
      "The number of words in chunk 224 is 100\n",
      "The number of words in chunk 225 is 100\n",
      "The number of words in chunk 226 is 100\n",
      "The number of words in chunk 227 is 100\n",
      "The number of words in chunk 228 is 100\n",
      "The number of words in chunk 229 is 100\n",
      "The number of words in chunk 230 is 100\n",
      "The number of words in chunk 231 is 100\n",
      "The number of words in chunk 232 is 100\n",
      "The number of words in chunk 233 is 100\n",
      "The number of words in chunk 234 is 100\n",
      "The number of words in chunk 235 is 100\n",
      "The number of words in chunk 236 is 100\n",
      "The number of words in chunk 237 is 100\n",
      "The number of words in chunk 238 is 100\n",
      "The number of words in chunk 239 is 100\n",
      "The number of words in chunk 240 is 100\n",
      "The number of words in chunk 241 is 100\n",
      "The number of words in chunk 242 is 100\n",
      "The number of words in chunk 243 is 100\n",
      "The number of words in chunk 244 is 100\n",
      "The number of words in chunk 245 is 100\n",
      "The number of words in chunk 246 is 100\n",
      "The number of words in chunk 247 is 100\n",
      "The number of words in chunk 248 is 100\n",
      "The number of words in chunk 249 is 100\n",
      "The number of words in chunk 250 is 100\n",
      "The number of words in chunk 251 is 100\n",
      "The number of words in chunk 252 is 100\n",
      "The number of words in chunk 253 is 100\n",
      "The number of words in chunk 254 is 100\n",
      "The number of words in chunk 255 is 100\n",
      "The number of words in chunk 256 is 100\n",
      "The number of words in chunk 257 is 100\n",
      "The number of words in chunk 258 is 100\n",
      "The number of words in chunk 259 is 100\n",
      "The number of words in chunk 260 is 100\n",
      "The number of words in chunk 261 is 100\n",
      "The number of words in chunk 262 is 100\n",
      "The number of words in chunk 263 is 100\n",
      "The number of words in chunk 264 is 100\n",
      "The number of words in chunk 265 is 100\n",
      "The number of words in chunk 266 is 100\n",
      "The number of words in chunk 267 is 100\n",
      "The number of words in chunk 268 is 100\n",
      "The number of words in chunk 269 is 100\n",
      "The number of words in chunk 270 is 100\n",
      "The number of words in chunk 271 is 100\n",
      "The number of words in chunk 272 is 100\n",
      "The number of words in chunk 273 is 100\n",
      "The number of words in chunk 274 is 100\n",
      "The number of words in chunk 275 is 100\n",
      "The number of words in chunk 276 is 100\n",
      "The number of words in chunk 277 is 100\n",
      "The number of words in chunk 278 is 100\n",
      "The number of words in chunk 279 is 100\n",
      "The number of words in chunk 280 is 100\n",
      "The number of words in chunk 281 is 100\n",
      "The number of words in chunk 282 is 100\n",
      "The number of words in chunk 283 is 100\n",
      "The number of words in chunk 284 is 100\n",
      "The number of words in chunk 285 is 100\n",
      "The number of words in chunk 286 is 100\n",
      "The number of words in chunk 287 is 100\n",
      "The number of words in chunk 288 is 100\n",
      "The number of words in chunk 289 is 100\n",
      "The number of words in chunk 290 is 100\n",
      "The number of words in chunk 291 is 100\n",
      "The number of words in chunk 292 is 100\n",
      "The number of words in chunk 293 is 100\n",
      "The number of words in chunk 294 is 100\n",
      "The number of words in chunk 295 is 100\n",
      "The number of words in chunk 296 is 100\n",
      "The number of words in chunk 297 is 100\n",
      "The number of words in chunk 298 is 100\n",
      "The number of words in chunk 299 is 100\n",
      "The number of words in chunk 300 is 100\n",
      "The number of words in chunk 301 is 100\n",
      "The number of words in chunk 302 is 100\n",
      "The number of words in chunk 303 is 100\n",
      "The number of words in chunk 304 is 100\n",
      "The number of words in chunk 305 is 100\n",
      "The number of words in chunk 306 is 100\n",
      "The number of words in chunk 307 is 100\n",
      "The number of words in chunk 308 is 100\n",
      "The number of words in chunk 309 is 100\n",
      "The number of words in chunk 310 is 100\n",
      "The number of words in chunk 311 is 100\n",
      "The number of words in chunk 312 is 100\n",
      "The number of words in chunk 313 is 100\n",
      "The number of words in chunk 314 is 100\n",
      "The number of words in chunk 315 is 100\n",
      "The number of words in chunk 316 is 100\n",
      "The number of words in chunk 317 is 100\n",
      "The number of words in chunk 318 is 100\n",
      "The number of words in chunk 319 is 100\n",
      "The number of words in chunk 320 is 100\n",
      "The number of words in chunk 321 is 100\n",
      "The number of words in chunk 322 is 100\n",
      "The number of words in chunk 323 is 100\n",
      "The number of words in chunk 324 is 100\n",
      "The number of words in chunk 325 is 100\n",
      "The number of words in chunk 326 is 100\n",
      "The number of words in chunk 327 is 100\n",
      "The number of words in chunk 328 is 100\n",
      "The number of words in chunk 329 is 100\n",
      "The number of words in chunk 330 is 100\n",
      "The number of words in chunk 331 is 100\n",
      "The number of words in chunk 332 is 100\n",
      "The number of words in chunk 333 is 100\n",
      "The number of words in chunk 334 is 100\n",
      "The number of words in chunk 335 is 100\n",
      "The number of words in chunk 336 is 100\n",
      "The number of words in chunk 337 is 100\n",
      "The number of words in chunk 338 is 100\n",
      "The number of words in chunk 339 is 100\n",
      "The number of words in chunk 340 is 100\n",
      "The number of words in chunk 341 is 100\n",
      "The number of words in chunk 342 is 100\n",
      "The number of words in chunk 343 is 100\n",
      "The number of words in chunk 344 is 100\n",
      "The number of words in chunk 345 is 100\n",
      "The number of words in chunk 346 is 100\n",
      "The number of words in chunk 347 is 100\n",
      "The number of words in chunk 348 is 100\n",
      "The number of words in chunk 349 is 100\n",
      "The number of words in chunk 350 is 100\n",
      "The number of words in chunk 351 is 100\n",
      "The number of words in chunk 352 is 100\n",
      "The number of words in chunk 353 is 100\n",
      "The number of words in chunk 354 is 100\n",
      "The number of words in chunk 355 is 100\n",
      "The number of words in chunk 356 is 100\n",
      "The number of words in chunk 357 is 100\n",
      "The number of words in chunk 358 is 100\n",
      "The number of words in chunk 359 is 100\n",
      "The number of words in chunk 360 is 100\n",
      "The number of words in chunk 361 is 100\n",
      "The number of words in chunk 362 is 100\n",
      "The number of words in chunk 363 is 100\n",
      "The number of words in chunk 364 is 100\n",
      "The number of words in chunk 365 is 100\n",
      "The number of words in chunk 366 is 100\n",
      "The number of words in chunk 367 is 100\n",
      "The number of words in chunk 368 is 100\n",
      "The number of words in chunk 369 is 100\n",
      "The number of words in chunk 370 is 100\n",
      "The number of words in chunk 371 is 100\n",
      "The number of words in chunk 372 is 100\n",
      "The number of words in chunk 373 is 100\n",
      "The number of words in chunk 374 is 100\n",
      "The number of words in chunk 375 is 100\n",
      "The number of words in chunk 376 is 100\n",
      "The number of words in chunk 377 is 100\n",
      "The number of words in chunk 378 is 100\n",
      "The number of words in chunk 379 is 100\n",
      "The number of words in chunk 380 is 100\n",
      "The number of words in chunk 381 is 100\n",
      "The number of words in chunk 382 is 100\n",
      "The number of words in chunk 383 is 100\n",
      "The number of words in chunk 384 is 100\n",
      "The number of words in chunk 385 is 100\n",
      "The number of words in chunk 386 is 100\n",
      "The number of words in chunk 387 is 100\n",
      "The number of words in chunk 388 is 100\n",
      "The number of words in chunk 389 is 100\n",
      "The number of words in chunk 390 is 100\n",
      "The number of words in chunk 391 is 100\n",
      "The number of words in chunk 392 is 100\n",
      "The number of words in chunk 393 is 100\n",
      "The number of words in chunk 394 is 100\n",
      "The number of words in chunk 395 is 100\n",
      "The number of words in chunk 396 is 100\n",
      "The number of words in chunk 397 is 100\n",
      "The number of words in chunk 398 is 100\n",
      "The number of words in chunk 399 is 100\n",
      "The number of words in chunk 400 is 100\n",
      "The number of words in chunk 401 is 100\n",
      "The number of words in chunk 402 is 100\n",
      "The number of words in chunk 403 is 100\n",
      "The number of words in chunk 404 is 100\n",
      "The number of words in chunk 405 is 100\n",
      "The number of words in chunk 406 is 100\n",
      "The number of words in chunk 407 is 100\n",
      "The number of words in chunk 408 is 100\n",
      "The number of words in chunk 409 is 100\n",
      "The number of words in chunk 410 is 100\n",
      "The number of words in chunk 411 is 100\n",
      "The number of words in chunk 412 is 100\n",
      "The number of words in chunk 413 is 100\n",
      "The number of words in chunk 414 is 100\n",
      "The number of words in chunk 415 is 100\n",
      "The number of words in chunk 416 is 100\n",
      "The number of words in chunk 417 is 100\n",
      "The number of words in chunk 418 is 100\n",
      "The number of words in chunk 419 is 100\n",
      "The number of words in chunk 420 is 100\n",
      "The number of words in chunk 421 is 100\n",
      "The number of words in chunk 422 is 100\n",
      "The number of words in chunk 423 is 100\n",
      "The number of words in chunk 424 is 100\n",
      "The number of words in chunk 425 is 100\n",
      "The number of words in chunk 426 is 100\n",
      "The number of words in chunk 427 is 100\n",
      "The number of words in chunk 428 is 100\n",
      "The number of words in chunk 429 is 100\n",
      "The number of words in chunk 430 is 100\n",
      "The number of words in chunk 431 is 100\n",
      "The number of words in chunk 432 is 100\n",
      "The number of words in chunk 433 is 100\n",
      "The number of words in chunk 434 is 100\n",
      "The number of words in chunk 435 is 100\n",
      "The number of words in chunk 436 is 100\n",
      "The number of words in chunk 437 is 100\n",
      "The number of words in chunk 438 is 100\n",
      "The number of words in chunk 439 is 100\n",
      "The number of words in chunk 440 is 100\n",
      "The number of words in chunk 441 is 100\n",
      "The number of words in chunk 442 is 100\n",
      "The number of words in chunk 443 is 100\n",
      "The number of words in chunk 444 is 100\n",
      "The number of words in chunk 445 is 100\n",
      "The number of words in chunk 446 is 100\n",
      "The number of words in chunk 447 is 100\n",
      "The number of words in chunk 448 is 100\n",
      "The number of words in chunk 449 is 100\n",
      "The number of words in chunk 450 is 100\n",
      "The number of words in chunk 451 is 100\n",
      "The number of words in chunk 452 is 100\n",
      "The number of words in chunk 453 is 100\n",
      "The number of words in chunk 454 is 100\n",
      "The number of words in chunk 455 is 100\n",
      "The number of words in chunk 456 is 100\n",
      "The number of words in chunk 457 is 100\n",
      "The number of words in chunk 458 is 100\n",
      "The number of words in chunk 459 is 100\n",
      "The number of words in chunk 460 is 100\n",
      "The number of words in chunk 461 is 100\n",
      "The number of words in chunk 462 is 100\n",
      "The number of words in chunk 463 is 100\n",
      "The number of words in chunk 464 is 100\n",
      "The number of words in chunk 465 is 100\n",
      "The number of words in chunk 466 is 100\n",
      "The number of words in chunk 467 is 100\n",
      "The number of words in chunk 468 is 100\n",
      "The number of words in chunk 469 is 100\n",
      "The number of words in chunk 470 is 100\n",
      "The number of words in chunk 471 is 100\n",
      "The number of words in chunk 472 is 100\n",
      "The number of words in chunk 473 is 100\n",
      "The number of words in chunk 474 is 100\n",
      "The number of words in chunk 475 is 100\n",
      "The number of words in chunk 476 is 100\n",
      "The number of words in chunk 477 is 100\n",
      "The number of words in chunk 478 is 100\n",
      "The number of words in chunk 479 is 100\n",
      "The number of words in chunk 480 is 100\n",
      "The number of words in chunk 481 is 100\n",
      "The number of words in chunk 482 is 100\n",
      "The number of words in chunk 483 is 100\n",
      "The number of words in chunk 484 is 100\n",
      "The number of words in chunk 485 is 100\n",
      "The number of words in chunk 486 is 100\n",
      "The number of words in chunk 487 is 100\n",
      "The number of words in chunk 488 is 100\n",
      "The number of words in chunk 489 is 100\n",
      "The number of words in chunk 490 is 100\n",
      "The number of words in chunk 491 is 100\n",
      "The number of words in chunk 492 is 100\n",
      "The number of words in chunk 493 is 100\n",
      "The number of words in chunk 494 is 100\n",
      "The number of words in chunk 495 is 100\n",
      "The number of words in chunk 496 is 100\n",
      "The number of words in chunk 497 is 100\n",
      "The number of words in chunk 498 is 100\n",
      "The number of words in chunk 499 is 100\n",
      "The number of words in chunk 500 is 100\n",
      "The number of words in chunk 501 is 100\n",
      "The number of words in chunk 502 is 100\n",
      "The number of words in chunk 503 is 100\n",
      "The number of words in chunk 504 is 100\n",
      "The number of words in chunk 505 is 100\n",
      "The number of words in chunk 506 is 100\n",
      "The number of words in chunk 507 is 100\n",
      "The number of words in chunk 508 is 100\n",
      "The number of words in chunk 509 is 100\n",
      "The number of words in chunk 510 is 100\n",
      "The number of words in chunk 511 is 100\n",
      "The number of words in chunk 512 is 100\n",
      "The number of words in chunk 513 is 100\n",
      "The number of words in chunk 514 is 100\n",
      "The number of words in chunk 515 is 100\n",
      "The number of words in chunk 516 is 100\n",
      "The number of words in chunk 517 is 100\n",
      "The number of words in chunk 518 is 100\n",
      "The number of words in chunk 519 is 100\n",
      "The number of words in chunk 520 is 100\n",
      "The number of words in chunk 521 is 100\n",
      "The number of words in chunk 522 is 100\n",
      "The number of words in chunk 523 is 100\n",
      "The number of words in chunk 524 is 100\n",
      "The number of words in chunk 525 is 100\n",
      "The number of words in chunk 526 is 100\n",
      "The number of words in chunk 527 is 100\n",
      "The number of words in chunk 528 is 100\n",
      "The number of words in chunk 529 is 100\n",
      "The number of words in chunk 530 is 100\n",
      "The number of words in chunk 531 is 100\n",
      "The number of words in chunk 532 is 100\n",
      "The number of words in chunk 533 is 100\n",
      "The number of words in chunk 534 is 100\n",
      "The number of words in chunk 535 is 100\n",
      "The number of words in chunk 536 is 100\n",
      "The number of words in chunk 537 is 100\n",
      "The number of words in chunk 538 is 100\n",
      "The number of words in chunk 539 is 100\n",
      "The number of words in chunk 540 is 100\n",
      "The number of words in chunk 541 is 100\n",
      "The number of words in chunk 542 is 100\n",
      "The number of words in chunk 543 is 100\n",
      "The number of words in chunk 544 is 100\n",
      "The number of words in chunk 545 is 100\n",
      "The number of words in chunk 546 is 100\n",
      "The number of words in chunk 547 is 100\n",
      "The number of words in chunk 548 is 100\n",
      "The number of words in chunk 549 is 100\n",
      "The number of words in chunk 550 is 100\n",
      "The number of words in chunk 551 is 100\n",
      "The number of words in chunk 552 is 100\n",
      "The number of words in chunk 553 is 100\n",
      "The number of words in chunk 554 is 100\n",
      "The number of words in chunk 555 is 100\n",
      "The number of words in chunk 556 is 100\n",
      "The number of words in chunk 557 is 100\n",
      "The number of words in chunk 558 is 100\n",
      "The number of words in chunk 559 is 100\n",
      "The number of words in chunk 560 is 100\n",
      "The number of words in chunk 561 is 100\n",
      "The number of words in chunk 562 is 100\n",
      "The number of words in chunk 563 is 100\n",
      "The number of words in chunk 564 is 100\n",
      "The number of words in chunk 565 is 100\n",
      "The number of words in chunk 566 is 100\n",
      "The number of words in chunk 567 is 100\n",
      "The number of words in chunk 568 is 100\n",
      "The number of words in chunk 569 is 100\n",
      "The number of words in chunk 570 is 100\n",
      "The number of words in chunk 571 is 100\n",
      "The number of words in chunk 572 is 100\n",
      "The number of words in chunk 573 is 100\n",
      "The number of words in chunk 574 is 100\n",
      "The number of words in chunk 575 is 100\n",
      "The number of words in chunk 576 is 100\n",
      "The number of words in chunk 577 is 100\n",
      "The number of words in chunk 578 is 100\n",
      "The number of words in chunk 579 is 100\n",
      "The number of words in chunk 580 is 100\n",
      "The number of words in chunk 581 is 100\n",
      "The number of words in chunk 582 is 100\n",
      "The number of words in chunk 583 is 100\n",
      "The number of words in chunk 584 is 100\n",
      "The number of words in chunk 585 is 100\n",
      "The number of words in chunk 586 is 100\n",
      "The number of words in chunk 587 is 100\n",
      "The number of words in chunk 588 is 100\n",
      "The number of words in chunk 589 is 100\n",
      "The number of words in chunk 590 is 100\n",
      "The number of words in chunk 591 is 100\n",
      "The number of words in chunk 592 is 100\n",
      "The number of words in chunk 593 is 100\n",
      "The number of words in chunk 594 is 100\n",
      "The number of words in chunk 595 is 100\n",
      "The number of words in chunk 596 is 100\n",
      "The number of words in chunk 597 is 100\n",
      "The number of words in chunk 598 is 100\n",
      "The number of words in chunk 599 is 100\n",
      "The number of words in chunk 600 is 100\n",
      "The number of words in chunk 601 is 100\n",
      "The number of words in chunk 602 is 100\n",
      "The number of words in chunk 603 is 100\n",
      "The number of words in chunk 604 is 100\n",
      "The number of words in chunk 605 is 100\n",
      "The number of words in chunk 606 is 100\n",
      "The number of words in chunk 607 is 100\n",
      "The number of words in chunk 608 is 100\n",
      "The number of words in chunk 609 is 100\n",
      "The number of words in chunk 610 is 100\n",
      "The number of words in chunk 611 is 100\n",
      "The number of words in chunk 612 is 100\n",
      "The number of words in chunk 613 is 100\n",
      "The number of words in chunk 614 is 100\n",
      "The number of words in chunk 615 is 100\n",
      "The number of words in chunk 616 is 100\n",
      "The number of words in chunk 617 is 100\n",
      "The number of words in chunk 618 is 100\n",
      "The number of words in chunk 619 is 100\n",
      "The number of words in chunk 620 is 100\n",
      "The number of words in chunk 621 is 100\n",
      "The number of words in chunk 622 is 100\n",
      "The number of words in chunk 623 is 100\n",
      "The number of words in chunk 624 is 100\n",
      "The number of words in chunk 625 is 100\n",
      "The number of words in chunk 626 is 100\n",
      "The number of words in chunk 627 is 100\n",
      "The number of words in chunk 628 is 100\n",
      "The number of words in chunk 629 is 100\n",
      "The number of words in chunk 630 is 100\n",
      "The number of words in chunk 631 is 100\n",
      "The number of words in chunk 632 is 100\n",
      "The number of words in chunk 633 is 100\n",
      "The number of words in chunk 634 is 100\n",
      "The number of words in chunk 635 is 100\n",
      "The number of words in chunk 636 is 100\n",
      "The number of words in chunk 637 is 100\n",
      "The number of words in chunk 638 is 100\n",
      "The number of words in chunk 639 is 100\n",
      "The number of words in chunk 640 is 100\n",
      "The number of words in chunk 641 is 100\n",
      "The number of words in chunk 642 is 100\n",
      "The number of words in chunk 643 is 100\n",
      "The number of words in chunk 644 is 100\n",
      "The number of words in chunk 645 is 100\n",
      "The number of words in chunk 646 is 100\n",
      "The number of words in chunk 647 is 100\n",
      "The number of words in chunk 648 is 100\n",
      "The number of words in chunk 649 is 100\n",
      "The number of words in chunk 650 is 100\n",
      "The number of words in chunk 651 is 100\n",
      "The number of words in chunk 652 is 100\n",
      "The number of words in chunk 653 is 100\n",
      "The number of words in chunk 654 is 100\n",
      "The number of words in chunk 655 is 100\n",
      "The number of words in chunk 656 is 100\n",
      "The number of words in chunk 657 is 100\n",
      "The number of words in chunk 658 is 100\n",
      "The number of words in chunk 659 is 100\n",
      "The number of words in chunk 660 is 100\n",
      "The number of words in chunk 661 is 100\n",
      "The number of words in chunk 662 is 100\n",
      "The number of words in chunk 663 is 100\n",
      "The number of words in chunk 664 is 100\n",
      "The number of words in chunk 665 is 100\n",
      "The number of words in chunk 666 is 100\n",
      "The number of words in chunk 667 is 100\n",
      "The number of words in chunk 668 is 100\n",
      "The number of words in chunk 669 is 100\n",
      "The number of words in chunk 670 is 100\n",
      "The number of words in chunk 671 is 100\n",
      "The number of words in chunk 672 is 100\n",
      "The number of words in chunk 673 is 100\n",
      "The number of words in chunk 674 is 100\n",
      "The number of words in chunk 675 is 100\n",
      "The number of words in chunk 676 is 100\n",
      "The number of words in chunk 677 is 100\n",
      "The number of words in chunk 678 is 100\n",
      "The number of words in chunk 679 is 100\n",
      "The number of words in chunk 680 is 100\n",
      "The number of words in chunk 681 is 100\n",
      "The number of words in chunk 682 is 100\n",
      "The number of words in chunk 683 is 100\n",
      "The number of words in chunk 684 is 100\n",
      "The number of words in chunk 685 is 100\n",
      "The number of words in chunk 686 is 100\n",
      "The number of words in chunk 687 is 100\n",
      "The number of words in chunk 688 is 100\n",
      "The number of words in chunk 689 is 100\n",
      "The number of words in chunk 690 is 100\n",
      "The number of words in chunk 691 is 100\n",
      "The number of words in chunk 692 is 100\n",
      "The number of words in chunk 693 is 100\n",
      "The number of words in chunk 694 is 100\n",
      "The number of words in chunk 695 is 100\n",
      "The number of words in chunk 696 is 100\n",
      "The number of words in chunk 697 is 100\n",
      "The number of words in chunk 698 is 100\n",
      "The number of words in chunk 699 is 100\n",
      "The number of words in chunk 700 is 100\n",
      "The number of words in chunk 701 is 100\n",
      "The number of words in chunk 702 is 100\n",
      "The number of words in chunk 703 is 100\n",
      "The number of words in chunk 704 is 100\n",
      "The number of words in chunk 705 is 100\n",
      "The number of words in chunk 706 is 100\n",
      "The number of words in chunk 707 is 100\n",
      "The number of words in chunk 708 is 100\n",
      "The number of words in chunk 709 is 100\n",
      "The number of words in chunk 710 is 100\n",
      "The number of words in chunk 711 is 100\n",
      "The number of words in chunk 712 is 100\n",
      "The number of words in chunk 713 is 100\n",
      "The number of words in chunk 714 is 100\n",
      "The number of words in chunk 715 is 100\n",
      "The number of words in chunk 716 is 100\n",
      "The number of words in chunk 717 is 100\n",
      "The number of words in chunk 718 is 100\n",
      "The number of words in chunk 719 is 100\n",
      "The number of words in chunk 720 is 100\n",
      "The number of words in chunk 721 is 100\n",
      "The number of words in chunk 722 is 100\n",
      "The number of words in chunk 723 is 100\n",
      "The number of words in chunk 724 is 100\n",
      "The number of words in chunk 725 is 100\n",
      "The number of words in chunk 726 is 100\n",
      "The number of words in chunk 727 is 100\n",
      "The number of words in chunk 728 is 100\n",
      "The number of words in chunk 729 is 100\n",
      "The number of words in chunk 730 is 100\n",
      "The number of words in chunk 731 is 100\n",
      "The number of words in chunk 732 is 100\n",
      "The number of words in chunk 733 is 100\n",
      "The number of words in chunk 734 is 100\n",
      "The number of words in chunk 735 is 100\n",
      "The number of words in chunk 736 is 100\n",
      "The number of words in chunk 737 is 100\n",
      "The number of words in chunk 738 is 100\n",
      "The number of words in chunk 739 is 100\n",
      "The number of words in chunk 740 is 100\n",
      "There are 740 chunks in total\n"
     ]
    }
   ],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "wordschunk = flatten_and_overlap(cleaneddoc)\n",
    "for i in wordschunk:\n",
    "    assert len(i) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'mind', 'wandering', 'defined', 'shifts', 'attention', 'task', 'related', 'processing', 'task', 'unrelated', 'thoughts', 'ubiquitous', 'phenomenon', 'negative', 'influence', 'performance', 'productivity', 'many', 'contexts', 'including', 'learning', 'propose', 'next', 'generation', 'learning', 'technologies', 'mechanism', 'detect', 'respond', 'mind', 'wandering', 'real', 'time', 'towards', 'end', 'developed', 'technology', 'automatically', 'detects', 'mind', 'wandering', 'eye', 'gaze', 'learning', 'instructional', 'texts', 'mind', 'wandering', 'detected', 'technology', 'intervenes', 'posing', 'time', 'questions', 'encouraging', 'reading', 'needed', 'multiple', 'rounds', 'iterative', 'refinement', 'summatively', 'compared', 'technology', 'yoked', 'control', 'experiment', 'participants', 'key', 'dependent', 'variable', 'performance', 'post', 'reading', 'comprehension', 'assessment', 'results', 'suggest', 'technology', 'successful', 'correcting', 'comprehension', 'deficits', 'attributed', 'mind', 'wandering', 'sigma', 'specific', 'conditions', 'thereby', 'highlighting', 'potential', 'improve', 'learning', 'attending', 'attention', 'keywords', 'mind', 'wandering']\n"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "print(wordschunck[0])\n",
    "#Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n",
    "\n",
    "#Sherin has 794 segements of text\n",
    "#Each segement becomes a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n",
    "\n",
    "# 1. In Sherin's paper, the interviewers' questions are removed from the text because these questions are not the interests of this research.\n",
    "#    Thus, I envision to remove certain part of the paper which is not considered as an interest of this research\n",
    "# 2. I think the title of each section, for example \"abstract\", \"method\", or \"conclusion\", can be misled the data, because these words are  \n",
    "#    consistent across all papers and can not be seen as the vocabulary that the authors want to express. Leaving those title in the text data\n",
    "#    may bias the direction of our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n",
    "\n",
    "# Vector and Matrix operations: transform each passage to a vector\n",
    "# Weight word frequency: to improve the result of vector, using a weighting function such as log-trasfromation to make the extreme comparatable.\n",
    "# Matrix normalization: making all elements, all vectors, in a matrix with length of 1\n",
    "# Deviation Vectors: substract the average normalized vector from each original vector, then normalize the new vector to obtain the deivationalized vector\n",
    "# Clustering: using hierarchical agglomerative clustering, we can iterate the process of reducing the total number of clusters by one until we get a single cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
